{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e701e7",
   "metadata": {},
   "source": [
    "# OpenAI Quickstart Guide\n",
    "\n",
    "From the official documentation available at: https://platform.openai.com/docs/overview\n",
    "\n",
    "You can use different models depending on your needs, check them at: https://platform.openai.com/docs/models\n",
    "\n",
    "Note: check the pricing before using a model! --> https://platform.openai.com/docs/pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c31972e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response = \n",
      "{'background': False,\n",
      " 'conversation': None,\n",
      " 'created_at': 1756758942.0,\n",
      " 'error': None,\n",
      " 'id': 'resp_68b6039e7fdc81a0b150d4d4922abe550f41a2da444eb905',\n",
      " 'incomplete_details': None,\n",
      " 'instructions': None,\n",
      " 'max_output_tokens': None,\n",
      " 'max_tool_calls': None,\n",
      " 'metadata': {},\n",
      " 'model': 'gpt-4.1-2025-04-14',\n",
      " 'object': 'response',\n",
      " 'output': [ResponseOutputMessage(id='msg_68b6039edeb481a0854054a272e196440f41a2da444eb905', content=[ResponseOutputText(annotations=[], text='Under the silvery moonlight, a gentle unicorn tiptoed through a field of stars, sprinkling sweet dreams over every sleeping child.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')],\n",
      " 'parallel_tool_calls': True,\n",
      " 'previous_response_id': None,\n",
      " 'prompt': None,\n",
      " 'prompt_cache_key': None,\n",
      " 'reasoning': Reasoning(effort=None, generate_summary=None, summary=None),\n",
      " 'safety_identifier': None,\n",
      " 'service_tier': 'default',\n",
      " 'status': 'completed',\n",
      " 'store': True,\n",
      " 'temperature': 1.0,\n",
      " 'text': ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'),\n",
      " 'tool_choice': 'auto',\n",
      " 'tools': [],\n",
      " 'top_logprobs': 0,\n",
      " 'top_p': 1.0,\n",
      " 'truncation': 'disabled',\n",
      " 'usage': ResponseUsage(input_tokens=18, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=29, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=47),\n",
      " 'user': None}\n",
      "\n",
      "response.output_text = \n",
      "Under the silvery moonlight, a gentle unicorn tiptoed through a field of stars, sprinkling sweet dreams over every sleeping child.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "from pprint import pprint\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Check if the API key is set\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set.\")\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = api_key\n",
    "\n",
    "# Initialize OpenAI client\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Make a request to the model\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "# Print the model's response\n",
    "print(\"Response = \")\n",
    "pprint(dict(response))\n",
    "print(f\"\\nresponse.output_text = \\n{response.output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6caaee",
   "metadata": {},
   "source": [
    "You can specify instructions to provide high-level instructions adopted as an overall context for your prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d7fecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with instructions = \n",
      "{'background': False,\n",
      " 'conversation': None,\n",
      " 'created_at': 1756760035.0,\n",
      " 'error': None,\n",
      " 'id': 'resp_68b607e34f208197ab5c49642b94e1840d7e0e684004ad84',\n",
      " 'incomplete_details': None,\n",
      " 'instructions': 'Answer in rhyme and with a cheerful style.',\n",
      " 'max_output_tokens': None,\n",
      " 'max_tool_calls': None,\n",
      " 'metadata': {},\n",
      " 'model': 'gpt-4.1-2025-04-14',\n",
      " 'object': 'response',\n",
      " 'output': [ResponseOutputMessage(id='msg_68b607e3e97c8197abf77607bb79c85d0d7e0e684004ad84', content=[ResponseOutputText(annotations=[], text='A butterfly flutters in sunlight so bright,  \\nWings painted with splashes of pure delight.  \\nIt dances on breezes, a soft, gentle flight,  \\nA whisper of wonder, a beautiful sight.\\n\\nFrom blossom to blossom it skips on the way,  \\nPainting the garden with colors of May.  \\nIt sips from the nectar, then giggles with glee,  \\nA speckled-winged artist as light as can be.\\n\\nWith silken wings open, it glimmers and spins,  \\nA jewel on a breeze where the magic begins!  \\nOh, butterfly darling, so lovely and free—  \\nKeep showing the world how joyful you’ll be!', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')],\n",
      " 'parallel_tool_calls': True,\n",
      " 'previous_response_id': None,\n",
      " 'prompt': None,\n",
      " 'prompt_cache_key': None,\n",
      " 'reasoning': Reasoning(effort=None, generate_summary=None, summary=None),\n",
      " 'safety_identifier': None,\n",
      " 'service_tier': 'default',\n",
      " 'status': 'completed',\n",
      " 'store': True,\n",
      " 'temperature': 1.0,\n",
      " 'text': ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'),\n",
      " 'tool_choice': 'auto',\n",
      " 'tools': [],\n",
      " 'top_logprobs': 0,\n",
      " 'top_p': 1.0,\n",
      " 'truncation': 'disabled',\n",
      " 'usage': ResponseUsage(input_tokens=27, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=137, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=164),\n",
      " 'user': None}\n",
      "\n",
      "response_with_instructions.output_text = \n",
      "A butterfly flutters in sunlight so bright,  \n",
      "Wings painted with splashes of pure delight.  \n",
      "It dances on breezes, a soft, gentle flight,  \n",
      "A whisper of wonder, a beautiful sight.\n",
      "\n",
      "From blossom to blossom it skips on the way,  \n",
      "Painting the garden with colors of May.  \n",
      "It sips from the nectar, then giggles with glee,  \n",
      "A speckled-winged artist as light as can be.\n",
      "\n",
      "With silken wings open, it glimmers and spins,  \n",
      "A jewel on a breeze where the magic begins!  \n",
      "Oh, butterfly darling, so lovely and free—  \n",
      "Keep showing the world how joyful you’ll be!\n"
     ]
    }
   ],
   "source": [
    "# Example of using the 'instructions' and 'reasoning' parameters in the model request\n",
    "response_with_instructions = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=\"Write a poem about a butterfly.\",\n",
    "    instructions=\"Answer in rhyme and with a cheerful style.\")\n",
    "\n",
    "# With some models you can also specify the reasoning effort parameter, e.g.    \n",
    "# reasoning={\"effort\": \"low\"})\n",
    "  \n",
    "print(\"Response with instructions = \")\n",
    "pprint(dict(response_with_instructions))\n",
    "print(f\"\\nresponse_with_instructions.output_text = \\n{response_with_instructions.output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd33373",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "Structured Output allows you to receive responses from the model in a predefined format, such as JSON or other structured data types. This is useful when you need the model's output to be machine-readable for further processing, integration, or automation. By specifying the desired structure, you can ensure consistency and make it easier to extract specific information from the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6410008c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math_reasoning = \n",
      "\n",
      "{'final_answer': 'x = -15/4 (which equals -3.75)',\n",
      " 'steps': [Step(explanation='Start with the equation and isolate the term containing x by subtracting 7 from both sides.', output='8x + 7 = -23  ⇒  8x = -23 - 7 = -30'),\n",
      "           Step(explanation='Now solve for x by dividing both sides by 8.', output='x = -30/8'),\n",
      "           Step(explanation='Simplify the fraction by dividing numerator and denominator by 2.', output='x = -15/4 = -3.75'),\n",
      "           Step(explanation='Check the solution by substituting x back into the original equation: compute 8x + 7 with x = -15/4.', output='8(−15/4) + 7 = −30 + 7 = −23, which matches the original right-hand side.')]}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"},\n",
    "    ],\n",
    "    text_format=MathReasoning,\n",
    ")\n",
    "\n",
    "math_reasoning = dict(response.output_parsed)\n",
    "print(\"math_reasoning = \\n\")\n",
    "pprint(math_reasoning)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
