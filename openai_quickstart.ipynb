{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e701e7",
   "metadata": {},
   "source": [
    "# OpenAI Quickstart Guide\n",
    "\n",
    "From the official documentation available at: https://platform.openai.com/docs/overview\n",
    "\n",
    "You can use different models depending on your needs, check them at: https://platform.openai.com/docs/models\n",
    "\n",
    "Note: check the pricing before using a model! --> https://platform.openai.com/docs/pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "from pprint import pprint\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Check if the API key is set\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set.\")\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = api_key\n",
    "\n",
    "# Initialize OpenAI client\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d97fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request to the model\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "# Print the model's response\n",
    "print(\"Response = \")\n",
    "pprint(dict(response))\n",
    "print(f\"\\nresponse.output_text = \\n{response.output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6caaee",
   "metadata": {},
   "source": [
    "You can specify instructions to provide high-level instructions adopted as an overall context for your prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d7fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the 'instructions' and 'reasoning' parameters in the model request\n",
    "response_with_instructions = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=\"Write a poem about a butterfly.\",\n",
    "    instructions=\"Answer in rhyme and with a cheerful style.\")\n",
    "\n",
    "# With some models you can also specify the reasoning effort parameter, e.g.    \n",
    "# reasoning={\"effort\": \"low\"})\n",
    "  \n",
    "print(\"Response with instructions = \")\n",
    "pprint(dict(response_with_instructions))\n",
    "print(f\"\\nresponse_with_instructions.output_text = \\n{response_with_instructions.output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd33373",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "Structured Output allows you to receive responses from the model in a predefined format, such as JSON or other structured data types. This is useful when you need the model's output to be machine-readable for further processing, integration, or automation. By specifying the desired structure, you can ensure consistency and make it easier to extract specific information from the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"},\n",
    "    ],\n",
    "    text_format=MathReasoning,\n",
    ")\n",
    "\n",
    "math_reasoning = dict(response.output_parsed)\n",
    "print(\"math_reasoning = \\n\")\n",
    "pprint(math_reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42b284",
   "metadata": {},
   "source": [
    "Or again you can use structured output to request the response to be in a specific format, e.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71083585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Alice and Bob are going to a science fair on Friday.\",\n",
    "        },\n",
    "    ],\n",
    "    text_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "print(\"response = \\n\")\n",
    "pprint(response.output_parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ad01a",
   "metadata": {},
   "source": [
    "When using Structured Outputs consider also to check for refusals and specify what to do in case of a refusal (i.e. when the model refuses to answer).\n",
    "\n",
    "Check the official documentation here --> https://platform.openai.com/docs/guides/structured-outputs#refusals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd1e7e",
   "metadata": {},
   "source": [
    "### Difference between `system`, `user`, and other roles in prompt content\n",
    "\n",
    "**System role:** The `system` message sets the behavior, context, or instructions for the model. It defines how the model should respond and can guide its tone, style, or constraints. For example, you can instruct the model to act as a math tutor or to answer in a specific format.\n",
    "\n",
    "**User role:** The `user` message represents the actual input or question from the end user. This is the prompt or query you want the model to answer.\n",
    "\n",
    "**Other roles (e.g., `assistant`):** Some APIs support additional roles like `assistant`, which can be used to provide previous model responses in a conversation, or custom roles for advanced workflows. These help maintain context in multi-turn conversations.\n",
    "\n",
    "In summary, `system` sets instructions/context, `user` provides the query, and other roles help structure multi-turn or complex interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3de64",
   "metadata": {},
   "source": [
    "# Image generation\n",
    "\n",
    "To generate images with OpenAI APIs, use the `image_generation` tool in your request. Specify your prompt in the `input` field and set the model (e.g., `\"gpt-5\"`). The API will return a base64-encoded image, which you can decode and save as a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55000fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "client = OpenAI() \n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Generate an image of gray tabby cat hugging an otter with an orange scarf\",\n",
    "    tools=[{\"type\": \"image_generation\"}],\n",
    ")\n",
    "\n",
    "# Save the image to a file\n",
    "image_data = [\n",
    "    output.result\n",
    "    for output in response.output\n",
    "    if output.type == \"image_generation_call\"\n",
    "]\n",
    "    \n",
    "if image_data:\n",
    "    image_base64 = image_data[0]\n",
    "    with open(\"otter.png\", \"wb\") as f:\n",
    "        f.write(base64.b64decode(image_base64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6797843",
   "metadata": {},
   "source": [
    "## Multi-turn image generation\n",
    "\n",
    "With the Responses API, you can build multi-turn conversations involving image generation either by providing image generation calls outputs within context (you can also just use the image ID), or by using the \n",
    "previous_response_id\n",
    "parameter. This makes it easy to iterate on images across multiple turnsâ€”refining prompts, applying new instructions, and evolving the visual output as the conversation progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f73e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Generate an image of gray tabby cat hugging an otter with an orange scarf\",\n",
    "    tools=[{\"type\": \"image_generation\"}],\n",
    ")\n",
    "\n",
    "image_data = [\n",
    "    output.result\n",
    "    for output in response.output\n",
    "    if output.type == \"image_generation_call\"\n",
    "]\n",
    "\n",
    "if image_data:\n",
    "    image_base64 = image_data[0]\n",
    "\n",
    "    with open(\"cat_and_otter.png\", \"wb\") as f:\n",
    "        f.write(base64.b64decode(image_base64))\n",
    "\n",
    "\n",
    "# Follow up\n",
    "\n",
    "response_fwup = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    previous_response_id=response.id,\n",
    "    input=\"Now make it look realistic\",\n",
    "    tools=[{\"type\": \"image_generation\"}],\n",
    ")\n",
    "\n",
    "image_data_fwup = [\n",
    "    output.result\n",
    "    for output in response_fwup.output\n",
    "    if output.type == \"image_generation_call\"\n",
    "]\n",
    "\n",
    "if image_data_fwup:\n",
    "    image_base64 = image_data_fwup[0]\n",
    "    with open(\"cat_and_otter_realistic.png\", \"wb\") as f:\n",
    "        f.write(base64.b64decode(image_base64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84af14c",
   "metadata": {},
   "source": [
    "You can also use streaming image generation to stream partial images as they are generated, if you are interested check here --> https://platform.openai.com/docs/guides/image-generation#streaming\n",
    "\n",
    "Moreover, note that when using some models (such as gpt-4.1) the model refines your prompt to enhance the result, if you are interested check here --> https://platform.openai.com/docs/guides/image-generation#revised-prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986e3fa",
   "metadata": {},
   "source": [
    "## Create a new image using image references\n",
    "\n",
    "You can create an image by using one or more other images as reference.\n",
    "\n",
    "With the Responses API, you can provide input images in 2 different ways:\n",
    "\n",
    "- By providing an image as a Base64-encoded data URL\n",
    "- By providing a file ID (created with the Files API)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b9fc7",
   "metadata": {},
   "source": [
    "### Creating a Base64-encoded data URL from an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd0a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def create_file(file_path):\n",
    "  with open(file_path, \"rb\") as file_content:\n",
    "    result = client.files.create(\n",
    "        file=file_content,\n",
    "        purpose=\"vision\",\n",
    "    )\n",
    "    return result.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106bfbd5",
   "metadata": {},
   "source": [
    "### Create a file ID from an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dca181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        base64_image = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    return base64_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01b92e7",
   "metadata": {},
   "source": [
    "Now, let us generate an image from these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de2ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = \"\"\"Generate a photorealistic image of a gift basket on a white background \n",
    "labeled 'Relax & Unwind' with a ribbon and handwriting-like font, \n",
    "containing all the items in the reference pictures.\"\"\"\n",
    "\n",
    "base64_image1 = encode_image(\"soap.png\")\n",
    "base64_image2 = encode_image(\"bath-bomb.png\")\n",
    "file_id1 = create_file(\"body-lotion.png\")\n",
    "file_id2 = create_file(\"incense-kit.png\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"input_text\", \"text\": prompt},\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image1}\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image2}\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"file_id\": file_id1,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"file_id\": file_id2,\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    tools=[{\"type\": \"image_generation\"}],\n",
    ")\n",
    "\n",
    "image_generation_calls = [\n",
    "    output\n",
    "    for output in response.output\n",
    "    if output.type == \"image_generation_call\"\n",
    "]\n",
    "\n",
    "image_data = [output.result for output in image_generation_calls]\n",
    "\n",
    "if image_data:\n",
    "    image_base64 = image_data[0]\n",
    "    with open(\"gift-basket.png\", \"wb\") as f:\n",
    "        f.write(base64.b64decode(image_base64))\n",
    "else:\n",
    "    print(response.output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcaec83",
   "metadata": {},
   "source": [
    "## Edit an image using a mask\n",
    "\n",
    "When editing an image you can also provide a mask to indicate where the image should be edited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d8960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "fileId = create_file(\"sunlit_lounge.png\")\n",
    "maskId = create_file(\"mask.png\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"generate an image of the same sunlit indoor lounge area with a pool but the pool should contain a flamingo\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"file_id\": fileId,\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"image_generation\",\n",
    "            \"quality\": \"high\",\n",
    "            \"input_image_mask\": {\n",
    "                \"file_id\": maskId,\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "image_data = [\n",
    "    output.result\n",
    "    for output in response.output\n",
    "    if output.type == \"image_generation_call\"\n",
    "]\n",
    "\n",
    "if image_data:\n",
    "    image_base64 = image_data[0]\n",
    "    with open(\"lounge.png\", \"wb\") as f:\n",
    "        f.write(base64.b64decode(image_base64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94689cf",
   "metadata": {},
   "source": [
    "## Increasing Input fidelity in image generation\n",
    "\n",
    "When dealing with images that require accurate preservation of elements (such as faces or logos) you can increase input fidelity by setting the <code>input_fidelity</code> parameter to <code>high</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f427198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "womanId = create_file(\"woman_futuristic.jpg\")\n",
    "logoId = create_file(\"brain_logo.png\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"input_text\", \"text\": \"Add the logo to the woman's top, as if stamped into the fabric.\"},\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"file_id\": womanId,\n",
    "                },\n",
    "                                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"file_id\": logoId,\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    tools=[{\"type\": \"image_generation\", \"input_fidelity\": \"high\"}],\n",
    ")\n",
    "\n",
    "# Extract the edited image\n",
    "image_data = [\n",
    "    output.result\n",
    "    for output in response.output\n",
    "    if output.type == \"image_generation_call\"\n",
    "]\n",
    "\n",
    "if image_data:\n",
    "    image_base64 = image_data[0]\n",
    "    with open(\"woman_with_logo.png\", \"wb\") as f:\n",
    "        f.write(base64.b64decode(image_base64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7ec23",
   "metadata": {},
   "source": [
    "## Additional custom options and features\n",
    "\n",
    "You can check for additional features and options such as the size, quality and the transparency here --> https://platform.openai.com/docs/guides/image-generation#size-and-quality-options  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed093ddc",
   "metadata": {},
   "source": [
    "# Analyze images\n",
    "\n",
    "You can use the vision capabilities of the model to analyze the content of an image, such as text or many other visual elements like shapes, colors, objects and textures.\n",
    "\n",
    "Input images must meet the following requirements to be used in the API.\n",
    "\n",
    "| Requirement         | Details                                                                                   |\n",
    "|---------------------|-------------------------------------------------------------------------------------------|\n",
    "| Supported file types| PNG (.png), JPEG (.jpeg, .jpg), WEBP (.webp), Non-animated GIF (.gif)                     |\n",
    "| Size limits         | Up to 50 MB total payload size per request<br>Up to 500 individual image inputs per request|\n",
    "| Other requirements  | No watermarks or logos<br>No NSFW content<br>Clear enough for a human to understand        |\n",
    "\n",
    "For more info check here --> https://platform.openai.com/docs/guides/images-vision#analyze-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbde65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    input=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"input_text\", \"text\": \"what's in this image?\"},\n",
    "            {\n",
    "                \"type\": \"input_image\",\n",
    "                \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "            },\n",
    "        ],\n",
    "    }],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b617d47",
   "metadata": {},
   "source": [
    "# Audio and speech\n",
    "\n",
    "You can manage audio while working with a model by:\n",
    "- having the model answer with a speech to a text prompt (text-to-speech)\n",
    "- having the model answer with a text to an audio prompt (speech-to-text)\n",
    "- having the model answer with a speech to an audio prompt (speech-to-speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4392db",
   "metadata": {},
   "source": [
    "## Text-to-speech\n",
    "\n",
    "Let us see how you can have the model answer with a speech to a text prompt (text-to-speech).\n",
    "\n",
    "For more info check here --> https://platform.openai.com/docs/guides/text-to-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15214b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-audio-preview\",\n",
    "    modalities=[\"text\", \"audio\"],\n",
    "    audio={\"voice\": \"alloy\", \"format\": \"wav\"},\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Is a golden retriever a good family dog?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "wav_bytes = base64.b64decode(completion.choices[0].message.audio.data)\n",
    "with open(\"dog.wav\", \"wb\") as f:\n",
    "    f.write(wav_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852ab11",
   "metadata": {},
   "source": [
    "Another use case for the text-to-speech is to generate spoken audio from input text, so let us do it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "speech_file_path = \"speech.mp3\"\n",
    "\n",
    "with client.audio.speech.with_streaming_response.create(\n",
    "    model=\"gpt-4o-mini-tts\",\n",
    "    voice=\"coral\",\n",
    "    input=\"Today is a wonderful day to build something people love!\",\n",
    "    instructions=\"Speak in a cheerful and positive tone.\",\n",
    ") as response:\n",
    "    response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ba49a",
   "metadata": {},
   "source": [
    "## Speech-to-text\n",
    "\n",
    "Let us see how you can have the model answer with a text to an audio prompt (speech-to-text).\n",
    "\n",
    "For more info check here --> https://platform.openai.com/docs/guides/speech-to-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ae76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Fetch the audio file and convert it to a base64 encoded string\n",
    "url = \"https://cdn.openai.com/API/docs/audio/alloy.wav\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "wav_data = response.content\n",
    "encoded_string = base64.b64encode(wav_data).decode('utf-8')\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-audio-preview\",\n",
    "    modalities=[\"text\", \"audio\"],\n",
    "    audio={\"voice\": \"alloy\", \"format\": \"wav\"},\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is in this recording?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_audio\",\n",
    "                    \"input_audio\": {\n",
    "                        \"data\": encoded_string,\n",
    "                        \"format\": \"wav\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "transcript = dict(completion.choices[0].message.audio)\n",
    "pprint(transcript['transcript'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548bc27b",
   "metadata": {},
   "source": [
    "Another use case for the speech-to-text is to transcribe an audio, so let us do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "audio_file= open(\"/path/to/file/audio.mp3\", \"rb\")\n",
    "\n",
    "transcription = client.audio.transcriptions.create(\n",
    "    model=\"gpt-4o-transcribe\", \n",
    "    file=audio_file\n",
    ")\n",
    "\n",
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa2a949",
   "metadata": {},
   "source": [
    "You can even produce a text which is the translation of the audio in another language!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3446ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "audio_file = open(\"/path/to/file/german.mp3\", \"rb\")\n",
    "\n",
    "translation = client.audio.translations.create(\n",
    "    model=\"whisper-1\", \n",
    "    file=audio_file,\n",
    ")\n",
    "\n",
    "print(translation.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa505b02",
   "metadata": {},
   "source": [
    "## Speech-to-speech\n",
    "\n",
    "Speech-to-speech can be achieved either by using a native speech-to-speech model or by chaining a speech-to-text and text-to-speech together.\n",
    "\n",
    "Due to its complexity, we omit this use case here. If you are interested you can check here --> https://platform.openai.com/docs/guides/voice-agents?voice-agent-architecture=speech-to-speech "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
